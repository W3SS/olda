% Template for ICASSP-2013 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{spconf,amsmath,graphicx}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{url}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{booktabs}
\usepackage{brian}

% Title.
% ------
\title{Learning to segment songs with ordinal linear discriminant analysis}
%
% Single address.
% ---------------
% \name{Author Names\thanks{Thanks to XYZ agency for funding.}}
% \address{Author Affiliations}
%
% For example:
% ------------
%\address{School\\
%   Department\\
%   Address}
%
% Two addresses (uncomment and modify for two-address case).
% ----------------------------------------------------------
\twoauthors%
 {Brian McFee\thanks{This work was supported by a grant from the Mellon foundation, and
grant IIS-1117015 from the National Science Foundation (NSF).}}{ Center for Jazz Studies\\
      Columbia University\\
  \texttt{brm2132@columbia.edu}}%
 {Daniel P.W. Ellis}{
  LabROSA, Department of Electrical Engineering\\
      Columbia University\\
  \texttt{dpwe@columbia.edu}}
%
\begin{document}
%\ninept
%
\maketitle
%
\begin{abstract}
This paper describes a supervised learning algorithm which optimizes a feature representation for temporally constrained clustering. 
The proposed method is applied to music segmentation, in which a song is partitioned into functional or
locally homogeneous segments (\eg, verse or chorus).  
To facilitate abstraction over multiple training examples, we develop a latent structural repetition feature, which summarizes the
repetitive structure of a song of any length in a fixed-dimensional representation.
Experimental results demonstrate that the proposed method efficiently integrates heterogeneous features, and improves segmentation accuracy.
\end{abstract}
%
\begin{keywords}
Music, automatic segmentation, learning
\end{keywords}
%
\section{Introduction}
\label{sec:intro}
% TODO:   2013-10-26 15:42:39 by Brian McFee <brm2132@columbia.edu>
% what is segmentation?
% how does it usually work?
% what's the problem with usual approaches

Automatic music segmentation algorithms take as input the acoustic signal of a musical performance, and produce a temporal
partitioning of the performance into a small number of \emph{segments}.  Ideally, segments correspond to structurally
meaningful regions of the performance, such \emph{verse} or \emph{chorus}.

Common approaches to music segmentation attempt to detect repeated patterns of features (\eg, a repeating chord progression),
often by some form of clustering~\cite{levy2008structural} or novelty detection~\cite{serra2012unsupervised}.  Often, features
are manually tuned and optimized for a specific development set, and carry implicit assumptions about the nature of musical 
structure.  As a concrete example, features built to detect repeated chord progressions may work well for characterizing some 
genres (\eg, rock or pop), but fail for other styles (\eg, jazz or hip-hop) which may be structured around timbre rather than melody.

In this work, we propose a supervised learning algorithm to automatically adapt acoustic and structural features to the statistics
of a training set. Given a collection of songs with structural annotations, the algorithm finds an optimal linear transformation
of features to preserve and predict segments.

\subsection{Our contributions}
Our primary contribution in this work is the ordinal linear discriminant analysis (OLDA) technique to learn a 
feature transformation which is optimized for musical segmentation, or more generally, time-series clustering.
As a secondary contribution, we propose a latent structural repetition descriptor, which facilitates learning and
generalization across multiple examples.

\subsection{Related work}
\label{sec:related}

The segmentation algorithm we use is most similar to the constrained clustering method of Levy and
Sandler~\cite{levy2008structural}, which incorporated sequential consistency constraints to a hidden Markov model. 
The method proposed here is simpler, and uses a sequentially constrained agglomerative clustering algorithm to 
produce a hierarchical segmentation over the entire track.  
Because the segmentation is hierarchical, the number of segments need not be specified in advance.

The proposed latent repetition features are adapted from the work of Serr\`{a}
\etal~\cite{serra2012unsupervised}. While qualitatively similar, we apply different filtering and
beat synchronization techniques to better preserve segment boundaries. 
In addition to chord sequence repetitions, our method includes timbre repetitions, as well as 
localized timbre, pitch, and timing information.

\section{Music segmentation}
\label{sec:features}
The criteria for deciding what is or is not a segment may vary across genres or styles.  
Pop music relies heavily on a verse/chorus structure, and is well characterized by repeating chord sequences. 
On the other hand, jazz tends to be structured by changing instrumentation (\eg, the current soloist),
and is better modeled as sequences of consistent timbre.
In general, a structural segmentation algorithm should include multiple feature representations in order to function
on various musical genres.

As a first step toward integrating multiple features, we introduce a structural repetition feature
which is amenable to learning and abstraction across multiple example songs.

\subsection{Latent structural repetition}
\Cref{fig:rep} outlines our approach for computing structural repetition features, which is adapted from
Serr\`{a} \etal~\cite{serra2012unsupervised}.  First, we extract beat-synchronous features (\eg, MFCCs or chroma) 
from the signal, and build a binary self-similarity matrix by linking each beat to its nearest neighbors in feature
space (\cref{fig:rep}, top-left). With beat-synchronous features, repeated sections appear as diagonals in the self-similarity
matrix. To detect repeated sections, the matrix is skewed by shifting the $i$th column down by $i$ rows
(\cref{fig:rep}, top-right), thereby converting diagonals into horizontals.

Nearest-neighbor linkage can result spurious links and skipped connections. 
Serr\`{a}~\etal{} resolve this by convolving with a Gaussian filter, which suppresses noise, but also blurs
segment boundaries. Instead, we use a horizontal median filter, which (for odd window length) produces a binary matrix,
suppresses links outside of repeated sequences, and fills in skipped connections (\cref{fig:rep},
bottom-left).  The width of the median filter directly corresponds to the minimal duration (in beats) of detected repetitions,
and is consequently easier to tune than a Gaussian filter.
The median filter also preserves edges better than the Gaussian filter, so we may expect more precise detection of segment boundaries.

Let $R \in \R^{2t \times t}$ denote the median-filtered, skewed self-similarity matrix over $t$ beats.  
% Because different songs have different durations, the dimensionality of $R$ varies. 
Because the dimensionality (number of rows) of $R$ varies from one track to the next,
% This makes 
it is difficult to model and generalize across collections.
However, what matters for segmentation is not the representation of the columns of $R$, but the similarity between them.
More precisely, methods which depend on distances between column-vectors --- \ie, those based on clustering or novelty curves --- 
are invariant to unitary transformations $U\trans$:
$
\|R_{\cdot, i} - R_{\cdot, j}\| = \|U\trans R_{\cdot, i} - U\trans R_{\cdot, j}\|.
$ 

% Rather than use $R$ directly, 
We therefore introduce \emph{latent structural repetition}, which compresses each
any song's $R$ matrix to a fixed-dimension representation.  Let $R = U\Sigma V\trans$ denote the singular value decomposition of $R$,
with (descending) singular values $\sigma_i$. 
The latent structural repetition feature is defined as the matrix $L$:
\begin{equation}
L \defeq \sigma_1^{-1} U\trans R = \sigma_1^{-1} \Sigma V\trans.\label{eq:latent-rep}
\end{equation}

Reducing $L$ to $d < 2t$ principal components retains the most important factors, and normalizing by $\sigma_1$ reduces 
the influence of track duration. 
\Cref{fig:rep} (bottom-right) depicts an example of the resulting features.  
Small values of $d$ often suffice to capture global structure: in the given
example, the top component suffices to detect transitions between verse 
(non-repetitive) and chorus (repetitive).

\begin{figure}
\centering%
\includegraphics[width=\columnwidth]{figs/rep}
\vspace{-2\baselineskip}
\caption{Repetition features derived from \emph{Tupac Shakur --- Trapped}. 
Top-left: a binary self-similarity ($k$-nearest-neighbor) matrix over beats.
Top-right: the time-lag transformation of the self-similarity matrix.
Bottom-left: the result of the horizontal median filter.
Bottom-right: 8-dimensional latent factor representation (best viewed in color).}
\label{fig:rep}
\end{figure}

\subsection{Constrained agglomerative clustering}
\label{sec:clustering}
Given a feature matrix $X \in \R^{D\times t}$, we produce a hierarchical clustering of the columns of $X$ by using the
linkage-constrained variant of Ward's agglomerative clustering algorithm~\cite{ward1963hierarchical} as implemented in
\texttt{scikit-learn}~\cite{pedregosa2011scikit}.  For each column $X_{\cdot,i}$, linkage constraints are generated
for $(i-1, i)$ and $(i, i+1)$.  Starting from $t$ clusters (one for each column of $X$), 
linked clusters are iteratively merged until only one remains.
The hierarchy is computed in $\Oh(t)$ merge operations, and due to the constraints, 
there are $\Oh(t)$ feasible merges at each step. 
Each merge operation takes time $\Oh(D + \log t)$ (to compute centroids and manage a priority queue), so the algorithm runs in $\Oh(tD + t\log t)$ time.
For $D \in \Omega(\log t)$, the cost of clustering is dominated by the $\Omega(tD)$ cost of computing $X$.

\subsection{Choosing the number of segments}
The hierarchical clustering produces segmentations for all numbers of segments $1 \leq k \leq t$.  
Because music segmentation is itself an ambiguous task, the ability to simultaneously produce segmentations at all resolutions is a key advantage of the 
proposed technique.  
However, standard evaluation procedures are defined only for flat segmentations, and require a specific pruning of the segment hierarchy.

To select the number of segments $k$, we compute the clustering cost of each pruning within a plausible bounded range 
$k_{\min} \leq k \leq k_{\max}$.  The bounds are determined by assuming average minimum and maximum segment duration 
of 10s and 45s.  AIC correction~\cite{akaike1973information} is then applied to each candidate pruning (assuming a spherical 
Gaussian model for each segment), and $k$ is chosen to minimize the AIC-corrected cost.

\subsection{Multiple features}
Structural repetition features are most commonly used to detect repeated chord sequences, which is appropriate for
segmenting many genres of popular music.  However, the conventions of structure can vary from one genre to the next,
so a general segmentation algorithm should include a variety of feature descriptors.
We therefore aggregate several types of feature descriptors for each beat:
\begin{description}\addtolength{\itemsep}{-0.35\baselineskip}%
\item[Timbre] mean Mel-frequency cepstral coefficients,
\item[Pitch] (coordinate-wise) median chroma vectors,
\item[Timbre repetition] latent MFCC repetitions,
\item[Pitch repetition] latent chroma repetitions,
\item[Time] Time-stamp (in seconds) of each beat, normalized time-stamps (as a fraction of track duration), 
beat indices $(1, 2, \ldots, t)$, and normalized beat indices\\ $(1/t, 2/t, \ldots, 1)$.
\end{description}
Local timbre features can be useful for non-repetitive forms (\eg, jazz), while timbre repetition is useful for sample-based genres (\eg, hip-hop or electonic). 
Similarly, chroma features capture local pitch similarity, while pitch repetitions capture chord progressions.
The time features act as an implicit quadratic regularization on segment durations, and promote balanced segmentations. 
We include normalized and unnormalized time-stamps to allow the learning algorithm (\Cref{sec:olda}) to adapt the regularization to 
either relative or absolute segment durations; beat index features differ from raw time features by correcting for tempo variation.

All features can be stacked together into a single feature matrix $X \in \R^{D\times t}$, and clustered via the method described
above. However, the relative magnitude and importance of each feature is not calibrated to produce optimal clusterings. 

\section{Ordinal linear discriminant analysis}
%\input{results-adj.tex}

\label{sec:olda}
To improve the feature representation for clustering, we propose a simple adaptation of Fisher's linear discriminant
analysis (FDA)~\cite{fisher1936use}.  In its multi-class form, FDA takes as input a labeled collection of data $x_i \in \R^D$
and class labels $y_i \in \{1,2,\ldots, C\}$, and produces a linear transformation $W \in \R^{D\times D}$ that simultaneously 
maximizes the distance between class centroids, and minimizes the variance of each class 
individually~\cite{fukunaga1990introduction}. This is accomplished by solving the following optimization:
\begin{equation}
W \defeq \argmax_W \trace\left( {(W\trans A_\text{W} W)}^{-1} W\trans A_\text{B} W \right) \label{eq:fda},
\end{equation}
where $A_\text{W}$ and $A_\text{B}$ are the \emph{within-} and \emph{between}-class scatter matrices:
\begin{align*}
A_\text{W} & \defeq \sum_c \sum_{i : y_i = c} (x_i - \mu_c)(x_i - \mu_c)\trans\\
A_\text{B} & \defeq \sum_c n_c (\mu_c - \mu)(\mu_c - \mu)\trans,
\end{align*}
$\mu$ denotes the mean across all classes, $\mu_c$ is the mean of class $c$, and $n_c$ denotes the number of examples in class $c$.
\Cref{eq:fda} can be efficiently solved as a generalized eigenvalue problem over the two scatter matrices $(A_\text{B},
A_\text{W})$~\cite{de2005eigenproblems}.

Class labels can be synthesized from segments on an annotated training song, so that the columns of $X$ belonging to the first 
segment are assigned to class 1, the second segment to class 2, and so on.
However, interpreting each segment as a distinct class could result in a repeated verse being treated as two distinct classes which 
cannot be separated. 
A more serious problem with this formulation is that it is unclear how to generalize across multiple songs, as it would result in
FDA attempting to separate segments from different songs.%, which is somewhat meaningless for the single-song segmentation task.

Due to linkage constraints, the agglomerative clustering algorithm (\cref{sec:clustering}) only considers merge 
operations over successive segments $(c, c+1)$. This motivates a relaxed FDA formulation which only attempts to separate adjacent
segments.  This is accomplished by replacing the between-class scatter matrix $A_B$ with the resulting \emph{ordinal scatter matrix}:
\begin{align*}
A_\text{O} &\defeq \sum_{c < C} n_c (\mu_c - \mu_{c+}) (\mu_c - \mu_{c+})\trans\\
            &\phantom{\defeq \sum_{c < C}} + n_{c+1} (\mu_{c+1} - \mu_{c+}) (\mu_{c+1} - \mu_{c+})\trans\\
\mu_{c+} &\defeq \frac{n_c \mu_c + n_{c+1} \mu_{c+1} }{n_c + n_{c+1}}.
\end{align*}
Intuitively, $A_\text{O}$ measures the deviation of successive segments $(c, c+1)$ from their mutual centroid $\mu_{c+}$, 
which is exactly the comparison performed for merge operations in the agglomerative clustering algorithm.
Optimizing $W$ to maximize this deviation, while minimizing within-segment variance, should enhance the overall segmentation accuracy.  
%While FDA has a similar objective, its relation to the linkage-constrained clustering objective is tenuous.

To improve numerical stability when $A_\text{W}$ is singular, we include a smoothing parameter
$\lambda > 0$.\footnote{The same regularization strategy is applied to FDA in \Cref{sec:eval}.} 
The OLDA optimization takes the form:
\begin{equation}
W \defeq \argmax_W \trace\left( {(W\trans (A_\text{W} + \lambda I) W)}^{-1} W\trans A_\text{O} W \right) \label{eq:olda},
\end{equation}
which again can be solved efficiently as a generalized eigenvalue problem over the matrix pair 
$(A_\text{O}, A_\text{W} + \lambda I)$.

Because interactions are only measured between neighboring segments, it is 
straightforward to include data from multiple songs by summing their individual contributions to $A_\text{O}$ and $A_\text{W}$.
After learning $W$, the feature matrix $X$ for a previously unseen song is transformed via $X \mapsto W\trans X$, and then 
clustered as described in \Cref{sec:clustering}.

\section{Evaluation}
\label{sec:eval}
All proposed methods are implemented in Python with the \texttt{librosa} package.\footnote{Code is
available at \url{https://github.com/bmcfee/olda}.}  
All signals were downsampled to 22KHz mono, and analyzed with a 93ms window and 3ms hop.  MFCCs are generated from 128 Mel bands
with an 8KHz cutoff. We take 32 MFCCs and 12 chroma bins; repetition features are calculated with $2\sqrt{t}$ nearest neighbors,
median-filtered with a window width of 7, and projected to 32 dimensions each.  Including the four time-stamp features, the combined
representation has dimension $D=112$. Beats were detected by the \emph{median-percussive} method~\cite{mcfee2014beat}.

\subsection{Data and metrics}
To evaluate the proposed methods, we evaluate predicted segmentations on two publicly available datasets:
\begin{description}\addtolength{\itemsep}{-0.25\baselineskip}%
\item[Beatles-ISO] 179 songs by the Beatles~\cite{harte2010towards,isophonicsbeatles}, and
\item[SALAMI-free] 253 songs from the SALAMI dataset~\cite{smith2011design} which are freely available on the 
Internet Archive~\cite{nieto2013convex}.
\end{description}
Both datasets provide labels for each annotated segment (\eg, \emph{verse} or \emph{intro}), but we ignore these
labels in this set of experiments. Compared to the Beatles corpus, SALAMI consists of tracks by multiple artists, 
and has much more diversity of genre, style, and instrumentation.

On both datasets, we compare to SMGA~\cite{serra2012unsupervised}, which achieved the
highest performance in the 2012 MIREX structural segmentation evaluation~\cite{Downie2008}.
On SALAMI-Free, we include comparisons to C-NMF~\cite{nieto2013convex} and SI-LPCA~\cite{weiss2011unsupervised}.

For both datasets, we evaluate the unweighted feature representation (\emph{Unweighted}), FDA optimization (using the
one-class-per-segment approach described in \Cref{sec:olda}), and OLDA.\@
To ensure fairness of evaluation, the FDA and OLDA models used on the Beatles-ISO were trained using only
SALAMI-free data, and vice versa.  FDA and OLDA were trained by optimizing $\lambda \in \{10^0, 10^1, \dots, 10^9\}$
to maximize $S_F$ score (see below) on the training set.

We report the following standard segmentation metrics:
\footnote{For a detailed description of segmentation metrics, see~\cite{mirexstructure}.}
\begin{description}\addtolength{\itemsep}{-0.25\baselineskip}%
% \item[Boundary retrieval] Precision, recall, and F-measure of segment boundary detection within a 0.5s or 3s window,
\item[Boundary retrieval] F-measure of segment boundary detection within a 0.5s or 3s window,
% \item[Normalized conditional entropy] (NCE) Over- and under-segmentation scores $S_O$ and $S_U$, and their harmonic
% mean $S_F$, and
\item[Normalized conditional entropy] (NCE) Harmonic mean ($S_F$) of over- and under-segmentation $S_O$ and $S_U$, 
% \item[Frame clustering] Precision ($P_C$), recall ($R_C$), and F-measure ($F_C$) of detecting whether any two frames 
% belong to the same segment.
\item[Frame clustering] F-measure ($F_C$) of detecting whether any two frames belong to the same segment.
\end{description}
Note that boundary retrieval and frame clustering metrics are sensitive to the number of
predicted segments, upon which multiple human annotators often disagree.
NCE is designed to be robust to changes in segmentation granularity.
Following MIREX practice, frames are sampled at a frequency of 10Hz for the NCE and frame clustering metrics. 


\subsection{Results}
\label{sec:results}
\input{results-adj-min.tex}

\Cref{tab:results} lists the results for the Beatles-ISO and SALAMI-free.
SMGA performs best\footnote{SMGA parameters were tuned to perform well on the Beatles data~\cite{serra2012unsupervised}.}
on most metrics for Beatles-ISO.\@
The NCE scores are qualitatively comparable between SMGA and OLDA ($S_F$ of 0.829 and 0.812, respectively).
The proposed methods achieve higher accuracy for boundary detection at 0.5s resolution, which can be attributed to the increased
precision afforded by the beat-synchronous and median-filtered repetition features.

On both datasets, OLDA consistently improves over the unweighted model, and achieves the highest scores on three out of four metrics for SALAMI-free.
% The results for FDA are mixed: while FDA performs comparably to OLDA on SALAMI-free, its performance degrades from the unweighted model on Beatles-ISO.\@
% By ignoring irrelevant segment comparisons, the OLDA model is better able to generalize from the multiple-artist training set than FDA.\@

\section{Conclusion}
\label{sec:conclusion}
This paper introduced the ordinal linear discriminant analysis (OLDA) method for learning feature projections to improve time-series clustering.
The proposed latent structural repetition features provide a convenient, fixed-dimensional representation of global song structure, which 
facilitates modeling across multiple songs.
The effective combination of global structural cues with local features results in significant improvement in segmentation accuracy 
on the mixed-genre SALAMI-free dataset.

\section{Acknowledgments}
The authors acknowledge support from The Andrew W. Mellon Foundation, and NSF grant IIS-1117015.

\bibliographystyle{IEEEbib}
\bibliography{refs}

\end{document}
