% -----------------------------------------------
% Template for MIREX 2010
% (based on ISMIR 2010 template)
% -----------------------------------------------

\documentclass{article}
\usepackage{mirex2010,amsmath,cite}
\usepackage{graphicx}
\usepackage{url}
\usepackage{cleveref}
\usepackage{booktabs}
\usepackage{brian}

% Title.
% ------
\title{DP1, MP1, MP2 entries for MIREX2013 structural segmentation and beat tracking}

% Single address
% To use with only one author or several with the same address
% ---------------
%\oneauthor
% {Names should be omitted for double-blind reviewing}
% {Affiliations should be omitted for double-blind reviewing}

% Two addresses
% --------------
\twoauthors%
 {Brian McFee} {Center for Jazz Studies \\ Columbia University \\ {\tt brm2132@columbia.edu}}
 {Daniel P.W. Ellis} {Electrical Engineering \\ Columbia University \\ {\tt dpwe@columbia.edu}}

% Three addresses
% --------------
% \threeauthors
%   {First author} {Affiliation1 \\ {\tt author1@music-ir.org}}
%   {Second author} {Affiliation2 \\ {\tt author2@music-ir.org}}
%   {Third author} {Affiliation3 \\ {\tt author3@music-ir.org}}

\begin{document}
%
\maketitle
%
\begin{abstract}
This extended abstract describes the algorithms for beat tracking and structural segmentation submitted as DP1 and MP1--2 to MIREX2013.
\end{abstract}
%
\section{Introduction}
\label{sec:introduction}

This document describes the implementation details for MIREX2013 submissions DP1 (beat tracking) and MP1--2
(structural segmentation).  All algorithms are implemented in Python, and code will be made publicly available
at the end of the evaluation.

Section 2 describes the beat tracking method (DP1), which is subsequently used in the segmentation method 
described in Section 3 (MP1).  Section 4 (DP2) describes a refinement of the MP1 method which uses a supervised
learning technique to optimally combine features for segmentation.

\Cref{tab:parameters} includes a summary of the parameters of the various components.

\begin{table}
\caption{Algorithm parameters and settings.\label{tab:parameters}}
\centering\begin{tabular}{lrr}
\toprule%
Parameter           & Symbol    & Value\\
\hline
Sampling rate (Hz)  & \em{sr}   & 22050\\
FFT size            & $N$       & 2048\\
Hop size            & $h$       & 64\\
\# Mel bins         & $d_M$       & 128\\
Max frequency (Hz)  & $f_{\max}$  & 8000\\
\\
\# MFCCs            & $d_t$     &   32\\
\# Chroma           & $d_p$     &   12\\
\# Repetition features      & $d_r$     &   32\\
Repetition window (beats)   & $\delta$  &   3\\
Repetition filter (beats)   & $w$       &   7\\
Repetition links (beats)    & $k$       &   $2\sqrt{n}$\\
\\
Minimum number of segments  & $K_{\min}$ & 8\\
Maximum number of segments  & $K_{\max}$ & 24\\
\bottomrule%
\end{tabular}
\end{table}

\section{DP1: Beat tracking}

The DP1 beat tracker is based on the dynamic programming method described by Ellis~\cite{ellis2007beat}.
The submitted version is based on the Python implementation provided as part of the {\tt librosa}\footnote{\url{http://github.com/bmcfee/librosa}} package, 
which differs from the original matlab implementation by including a frame offset
correction to compensate for the systemic lag observed by Davies \etal~\cite{davies2009evaluation}.

The beat tracker operates by solving a dynamic program to select peaks from an onset strength function.  
Given a log-magnitude Mel spectrogram $M$,~\cite{ellis2007beat} defines an onset strength function as the sum over
frequency bands of thresholded spectral difference:
\[
\omega_0(t) = \sum_f \max(0, M_{f, t} - M_{f, t-1}).
\]

The current implementation differs from the original in two ways.  First, the frequency resolution has been
increasing the sampling rate {\em sr} from 8000 to 22050, FFT size $N$ from 256 to 2048, and the number of Mel bins
$d_M$ from 40 to 128 (see \Cref{tab:parameters}).  Second, the onset detection function was modified to prefer
vertical synchronicity of onsets across frequency bins, rather than raw (log-) magnitude, by replacing the sum operator
with a median:
\[
\omega(t) = \median_f \max(0, M_{f,t} - M_{f, t-1}).
\]
The resulting onset strength function tends to be sparser, and less sensitive to high-magnitude asynchronous
events.  In practice, we have observed significant improvements in accuracy over the previous method.

\label{sec:beats}

\section{MP1: Unsupervised structural segmentation}

The submitted methods for structural segmentation are based upon agglomerative clustering of beat-synchronous
feature descriptors, similar to the method of Levy and Sandler~\cite{levy2008structural}.

For a given song, the general strategy is as follows:
\begin{enumerate}
\item Detect beats using the method of \Cref{sec:beats};
\item compute a beat-synchronous feature matrix $X \in \R^{D\times n}$ where $D$ denotes the number of features,
and $n$ denotes the number of beats;
\item perform temporally-constrained agglomerative clustering on the columns of $X$;
\item prune the cluster tree to $K$ segments.
\end{enumerate}


\subsection{Features}

For each song, we compute a variety of both local and global beat-level features, as follows.

\subsubsection{Timbre and pitch}
As a first step, we compute the top $d_M=32$ Mel frequency cepstral coefficients (MFCC) and chroma vector for each 
audio frame.  Across frames within each beat, the MFCCs are mean-aggregated, and the chroma vectors are 
median-aggregated.


\subsubsection{Repetition}
In addition the local timbre and pitch descriptors described above, we encode global repetition structure.  The
method is inspired by the method of Serr\`{a} \etal~\cite{serra2012unsupervised}, with several modifications.

The proposed method first computes a (possibly asymmetric) $k$-nearest-neighbor graph over beat-synchronous timbre 
descriptors $X$, resulting in a binary recurrence matrix $R$, where
\[
R_{i,j} = 1 \Leftrightarrow X_j \text{ is a nearest neighbor of } X_i.
\]
Links within $\pm\delta=\pm3$ beats of $i$ are suppressed, as they do not contribute meaningful global structure
information.  Each beat is connected to $k=2\sqrt{n}$ nearest neighbors.

The recurrence matrix is then skewed so that
\[
S_{i,j} = 1 \Leftrightarrow R_{i, j+i} = 1,
\]
effectively reparameterizing the recurrence matrix from time-time to time-lag.  Note that
unlike~\cite{serra2012unsupervised}, we do not wrap positive- and negative-lag on top of each-other, resulting
in a taller matrix $S \in \{0,1\}^{2n, n}$.

To correct for linkage errors, $S$ must be blurred or smoothed in some way.  Rather than applying a
2-dimensional Gaussian filter, we first observe that the use of beat-synchronous features significantly 
reduces the effects of tempo variation, so a vertical smoothing is unnecessary.  Instead, we apply a
1-dimensional median filter of width $w=7$ to each row of $S$.  The median filter more cleanly preserves
boundaries, and eliminates binary noise (skipped beats and spurious linkages).  The width of the filter also
carries an intuitive interpretation as the minimum segment duration to detect.

As a final step, the filtered time-lag matrix $S'$ is compressed to a latent structure representation by singular
value decomposition.  
Let $S' = U\Sigma V\trans$ denote the SVD; since $U$ is unitary, applying the transformation $S' \mapsto U\trans S'$ 
will not change distances or the induced clustering of columns.  Similarly, normalizing by the maximal 
singular value $\sigma_1$ does not change the induced clustering, but ensures that the latent repetition features
lie within a bounded region, regardless of the number of beats.  We therefore apply the following
transformation:
\[
S' \rightarrow \sigma_1^{-1} U\trans S',
\]
and project onto the top-$d_r = 32$ dimensions.  While the low-rank projection is not strictly necessary when
segmenting a single song, it ensures that the representation lies in a fixed-dimensional space, and is critical
when learning a feature weighting from multiple songs (\Cref{sec:supervised}).

Using the method described above, we compute two sets of latent structure features for each song: one based on
timbre similarity using the beat-synchronous MFCCs, and one based on history-stacked beat-synchronous chroma
vectors.

\subsubsection{Time and location}
Finally, we append the following four features to each beat: i) time (in seconds) of the beat, ii) normalized
time of the beat, iii) raw index of the beat (\eg, 0, 1, 2, \dots), iv) normalized beat index.  These features
add an implicit regularization to the clustering algorithm by adding a quadratic penalty to the duration of each
detected segment.


The final feature matrix $X$ consists of $D = d_t + d_p + 2d_r + 4 = 112$ features for each beat.

\subsection{Clustering and pruning}

Given a feature matrix $X \in \R^{D \times n}$, the columns are recursively merged via linkage-constrained
agglomerative clustering.  Specifically, we apply the Scikit-learn implementation of Ward's
method~\cite{pedregosa2011scikit, ward1963hierarchical}, with a constraints of the form $(t, t+1)$ for all $0
\leq t < n$.

The output of the clustering algorithm on $X$ is a cluster tree, \ie, a dendrogram over cluster refinements
induced by the iterative merging procedure.  The tree therefore encodes clusterings for all numbers of clusters 
$1 \leq K \leq n$.  The cluster tree is first simplified by pruning to at least $K_{\min}=8$ and at most 
$K_{\max} = 24$ clusters.

The final pruning of the resulting cluster tree is selected by optimizing an AIC-corrected score 
function~\cite{akaike1973information}.  Specifically, for a potential number of clusters $K$, the corresponding
clustering is selected from the tree, and for each segment $1 \leq i \leq K$, the centroid $\mu_i$ is estimated.  
This can be interpreted as fitting a sequence of isotropic Gaussian distributions $\N(\mu_i, I)$ to the selected 
columns $X_{[i]}$. Given this interpretation, we compute the average log-likelihood of the data ($X_{[i]}$) under 
each segment's model, and add the AIC correction $-K\cdot D$.  The optimal value is selected by searching over
$K_{\min} \leq K \leq K_{\max}$.

Given the final clustering of beat-synchronous features, the segment boundaries are detected by locating
cluster-label disagreements, and converting back to the time index of the corresponding beat frames.  The
0-marker and end-of-track time are included in the final segment predictions.

\section{MP2: Supervised segmentation}
\label{sec:supervised}

The quality of the clustering produced in the previous section will ultimately depend on the computation of
distances between columns of $X$.  In this section, we propose a method to optimize the distance function to
improve clustering.

The proposed method is based upon the multi-class variant of Fisher's linear discriminant
analysis (FDA)~\cite{fisher1936use, fukunaga1990introduction}.  In the context of segmentation, we consider a 
``class'' to be a segment identifier (though not it's structural label, so two different ``chorus'' segments map to
two different classes).
A structural segmentation annotation for a song can be converted into a labeled data set by first extracting the 
feature matrix $X \in \R^{D\times n}$, and then assigning each column $X_i$ to the segment $y_i$ which contains 
it.  In the usual multi-class FDA setting, the data would be transformed via the learned mapping $X \mapsto
W\trans X$,
where
\begin{equation}
W \defeq \argmin_W \trace\left( (W\trans S_\text{B} W)^{-1} W\trans S_\text{W} W \right) \label{eq:fda},
\end{equation}
where $S_\text{W}$ and $S_\text{B}$ are the \emph{within-} and \emph{between}-class scatter matrices:
\begin{align*}
S_\text{W} & \defeq \sum_k \sum_{i : y_i = k} (X_i - \mu_k)(X_i - \mu_k)\trans\\
S_\text{W} & \defeq \sum_k n_k (\mu_k - \mu)(\mu_k - \mu)\trans,
\end{align*}
and $n_k$ denotes the number of beats in segment $k$.
\Cref{eq:fda} is equivalent to a generalized eigenvalue problem, and can thus be solved efficiently.  Moreover,
the scatter matrices from multiple songs can be accumulated into $S_\text{W}$ and $S_\text{B}$, allowing the
method to generalize across large collections.

FDA generally tries to condense points within each class (segment), and spread the centroids of each class
(segment) far apart from each-other.  This would have the undesirable effect of attempting to spread the
centroids of, \eg, two chorus segments far apart from each-other.  To counteract this effect, we modified the
FDA objective to only spread means between sequentially adjacent segments $(k, k+1)$.  
The resulting \emph{ordinal} FDA algorithm optimizes
\begin{equation}
W \defeq \argmin_W \trace\left( (W\trans (S_\text{O} + \lambda I) W)^{-1} W\trans S_\text{W} W \right)
\label{eq:ofda},
\end{equation}
where 
\begin{align*}
S_\text{O} &\defeq \sum_{k < K} n_k (\mu_k - \mu_{k+}) (\mu_k - \mu_{k+})\trans\\
\mu_{k+} &\defeq \frac{n_k \mu_k + n_{k+1} \mu_{k+1} }{n_k + n_{k+1}},
\end{align*}
and $\lambda > 0$ is a regularization term which is selected by cross-validation on a labeled training set. As
in FDA, scatter matrices are aggregated across multiple songs.

In order to set $\lambda$ and learn $W$, we used a portion of the SALAMI dataset which is publicly available
on \url{http://archive.org/}~\cite{smith2011design}.  All annotations were extracted from the {\tt
parsed/textfile1\_functions.txt} files.

After applying the learned transformation $X \mapsto W\trans X$, the clustering and pruning steps follow just as 
in MP1.

\bibliography{refs}

\end{document}
